# 📘 Introduction to Generative AI (Google Cloud)
Presenter: Roger Martinez, Developer Relations Engineer at Google Cloud
Purpose: Understand what Generative AI is, how it works, and how it is applied.

## 🤖 What Is Generative AI?
Generative AI (GenAI) is a type of artificial intelligence that can create content—such as:

Text

Images

Audio

Synthetic data

Code

Video

GenAI models are trained on large datasets and generate new content based on learned patterns.

## 📚 Foundations: AI vs. ML vs. DL
🧩 **Artificial Intelligence (AI)**
A discipline of computer science focused on building systems that mimic human intelligence.

Includes reasoning, learning, acting autonomously.

🧠 **Machine Learning (ML)**
A subset of AI that enables systems to learn from data rather than explicit programming.

Produces models that generalize from training data.

⚙️ **Types of ML**
Supervised Learning: Uses labeled data. Model learns relationships between input (X) and output (Y).

Example: Predicting tips based on order type and bill total.

Unsupervised Learning: Uses unlabeled data to find patterns (e.g., clustering).

Example: Grouping employees based on tenure and income.

Semi-supervised Learning: Uses a mix of labeled + unlabeled data. Common in neural networks.

🧠 **Deep Learning (DL)**
A subset of ML using Artificial Neural Networks (ANNs) with multiple layers (deep networks).

Learns complex features.

Inspired by the human brain.

## 🧪 Generative vs. Discriminative Models
Type	Description	Example Output
Discriminative	Classifies input data (predicts label y given x).	Spam/Not Spam
Generative	Learns full data distribution (P(x, y)) and generates new data.	Text, images, etc.

Example:

Discriminative: Classifies a picture as a dog.

Generative: Creates a new image of a dog.

📈 **Traditional vs. Generative AI Workflows**
🏗️ **Traditional ML:**
Inputs: Training code + labeled data.

Outputs: Classification, prediction, clustering.

## 🎨 Generative AI:
Inputs: Training code + labeled & unlabeled data.

Output: New content (text, image, code, etc.)

Built on foundation models.

🧾 **Mathematical Framing**
Y = f(X) → Output is a function of inputs.

If Y is a number/class → traditional ML.

If Y is text/image/audio → it's Generative AI.

🏛️ **Foundation Models**
Large pre-trained models adaptable to many downstream tasks:

Text: Chat, summarization, sentiment analysis

Vision: Image captioning, object detection

Multimodal: Text + image/video/audio

💡 **Google’s Gemini and LaMDA are examples.**

## 📦 Types of Generative Models (Text Input)
Model Type	Description
Text-to-Text	Input text → Output text (e.g. translation, summarization)
Text-to-Image	Input text → Output image (e.g. diffusion models like Stable Diffusion)
Text-to-Video	Input text → Output video
Text-to-3D	Input text → Output 3D model
Text-to-Task	Input text → Task execution (e.g. UI automation, doc editing)

## 🛠️ Real-World Applications
🔤 **Language & Text**
Chatbots, email generation, documentation

Sentiment analysis

Text summarization

🧑‍💻 **Code Generation**
Convert code formats (e.g., Pandas DataFrame → JSON)

Debug or complete code

Generate API documentation

🎨 **Vision**
Image generation from text (Stable Diffusion)

Object recognition (e.g., occupancy analytics)

💼 **Industry Use Cases**
Healthcare: diagnostics, summarization

Finance: fraud detection

Retail: recommendation systems

Customer service: chatbots, virtual agents

🧪 **Transformers: The Backbone of GenAI**
Introduced in 2017/2018, revolutionizing NLP.

Components:

Encoder: Encodes input

Decoder: Predicts output

Core to LLMs (e.g., Gemini, GPT)

## ⚠️ Hallucinations
Nonsensical or incorrect model outputs

Caused by:

Limited or noisy training data

Poor context

Lack of constraints

## 🧠 Prompt Engineering
Prompt = Input to LLM to control output.

Prompt design = Crafting effective inputs to guide desired model behavior.

## 🧩 Summary: What is Generative AI?
Generative AI is a branch of AI that uses deep learning to create new content based on patterns learned from existing data.

Core Characteristics:
Learns from existing content

Outputs novel content

Uses foundation models

Fueled by transformer architecture

